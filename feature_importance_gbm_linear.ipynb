{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "original link: https://colab.research.google.com/drive/11zgTkprjo6gbW8ENSsrAPYSFZiSzGlRM?usp=sharing#scrollTo=aQckSErNmmns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using GBM to extract strong feature interactions and predict demmand in an interpretable manner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "from bisect import bisect_right\n",
    "import os \n",
    "import joblib\n",
    "import itertools\n",
    "import datetime\n",
    "from datetime import datetime as dt\n",
    "import random\n",
    "\n",
    "import pydotplus\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import linregress\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from dateutil.relativedelta import *\n",
    "import holidays\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from six import StringIO\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import _tree\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import xgboost as xgb\n",
    "import parfit.parfit as pf\n",
    "import xgbfir\n",
    "from featexp import get_trend_stats, get_grouped_data\n",
    "\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "seed_everything(42)\n",
    "\n",
    "sns.set()\n",
    "sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 4000\n",
    "pd.options.display.max_seq_items = 2000\n",
    "pd.options.display.float_format = '{:.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = ''\n",
    "file = ''\n",
    "data = pd.read_csv(path + file, parse_dates = ['validdate'])\n",
    "events = pd.read_csv(path+'', parse_dates=['validdate'])\n",
    "data = pd.merge(data, events, left_on = ['validdate', 'LOCATION_NAME'], right_on = ['validdate', 'LOCATION_NAME'])\n",
    "train = data[(data['DATE']>0) & (data['validdate']<'2020-05-02') & (data['validdate']>'2019-05-10')]\n",
    "test = data[data['validdate']>='2020-05-02']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Functions\n",
    "\n",
    "1. **apply_binning, get_tree, tree_cutpoints, get_readable_cutpoints:** functions for univariate supervised discretization based on regression trees, optimized by their depht via cross-validation.\n",
    "2. **encode_data_ohe:** one hot encoding for train and test preserving same columns, missing values handling.\n",
    "3. **add_interactions:** dumping trained xgboost model, parsing and cleaning interactions, filtering only stable interactions by Gain Rank, assigning interactions of depth 2,3,4 to original dataset.\n",
    "4. **draw_features, get_variable, get_variables:** functions for exploring binary rulesets with decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(y_true, y_pred):\n",
    "  # problem with MAPE as an error metric\n",
    "  y_true = np.array(y_true)\n",
    "  y_pred = np.array(y_pred)\n",
    "  return np.abs((y_true - y_pred)/y_true).mean()\n",
    "\n",
    "def get_tree(X, Y, variable, n_folds, param_grid, mape_scorer):\n",
    "\n",
    "    param_grid = param_grid\n",
    "    tree_model = GridSearchCV(DecisionTreeRegressor(random_state=0),\n",
    "                              cv = n_folds, \n",
    "                              scoring = mape_scorer,\n",
    "                              param_grid = param_grid)\n",
    "\n",
    "\n",
    "    tree_model.fit(X[[variable]], Y)\n",
    "    estimator = DecisionTreeRegressor(max_depth=tree_model.best_params_['max_depth'])\n",
    "    estimator.fit(X[[variable]], Y)\n",
    "    return estimator\n",
    "\n",
    "def tree_cutpoints(tree, feature_names):\n",
    "    cutpoints = []\n",
    "    d= []\n",
    "    tree_ = tree.tree_\n",
    "    n_nodes = tree.tree_.node_count\n",
    "    children_left = tree.tree_.children_left\n",
    "    children_right = tree.tree_.children_right\n",
    "    feature = tree.tree_.feature\n",
    "    threshold = tree.tree_.threshold\n",
    "    feature_name = [\n",
    "        feature_names[i] if i != _tree.TREE_UNDEFINED else \"undefined!\"\n",
    "        for i in tree_.feature\n",
    "    ]\n",
    "    # print(\"def tree({}):\".format(\", \".join(feature_names)))\n",
    "\n",
    "    def recurse(node, depth):\n",
    "        indent = \"  \" * depth\n",
    "        if tree_.feature[node] != _tree.TREE_UNDEFINED:\n",
    "            name = feature_name[node]\n",
    "            threshold = tree_.threshold[node]\n",
    "            # print(\"{}if {} <= {}:\".format(indent, name, threshold))\n",
    "            recurse(tree_.children_left[node], depth + 1)\n",
    "            # print(\"{}else:  # if {} > {}\".format(indent, name, threshold))\n",
    "            recurse(tree_.children_right[node], depth + 1)\n",
    "            \n",
    "            if (depth==tree.max_depth):\n",
    "                cutpoints.append(threshold)\n",
    "              \n",
    "        else:\n",
    "            pass\n",
    "            # print(\"{}return {}\".format(indent, tree_.value[node]))\n",
    "\n",
    "    recurse(0, 1)\n",
    "    return cutpoints\n",
    "\n",
    "def get_readable_cutpoints(val, binning):\n",
    "    \n",
    "    dicts = {}\n",
    "    binning.sort()\n",
    "    binning = np.round(binning, 6)\n",
    "\n",
    "    for i, cut in enumerate(binning):\n",
    "        \n",
    "        if ((val<=cut) & (i == 0)):\n",
    "            return 'lt_'+str(binning[i])\n",
    "        else:\n",
    "            if ((val>cut) & (i + 1 == len(binning))):\n",
    "                 return 'gt_'+str(binning[i])\n",
    "            else:\n",
    "                 if (val>cut) & (val<=binning[i+1]):\n",
    "                    return 'btw_'+str(binning[i])+'_and_'+str(binning[i+1])\n",
    "                \n",
    "def encode_float_tree(X_train, X_valid,  Y_train, use_features, numeric_col, target, n_folds, param_grid, scorer):\n",
    "    for variable in numeric_col:\n",
    "        \n",
    "        tree = get_tree(X_train, Y_train, variable, n_folds, param_grid, mape_scorer)\n",
    "        binning = tree_cutpoints(tree, [variable])\n",
    "        \n",
    "        X_train[variable] =  X_train[variable].apply(get_readable_cutpoints, binning = binning)\n",
    "        X_valid[variable] =  X_valid[variable].apply(get_readable_cutpoints, binning = binning)\n",
    "       \n",
    "    return X_train, X_valid\n",
    "\n",
    "\n",
    "def encode_data_ohe(X_train, X_test, Y_train, use_features, bool_col, cat_col):\n",
    "\n",
    "    X_train_enc = X_train[use_features].copy()\n",
    "    X_test_enc = X_test[use_features].copy()\n",
    "\n",
    "    X_train_enc = X_train_enc.replace([np.inf, -np.inf], np.nan)\n",
    "    X_test_enc = X_test_enc.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    X_train_enc[cat_col]=X_train_enc[cat_col].fillna('N/A')\n",
    "    X_test_enc[cat_col]=X_test_enc[cat_col].fillna('N/A')\n",
    "\n",
    "    X_train_enc[bool_col]=X_train_enc[bool_col].fillna(0).astype('float')\n",
    "    X_test_enc[bool_col]=X_test_enc[bool_col].fillna(0).astype('float')\n",
    "\n",
    "    X_train_ohe = pd.get_dummies(X_train_enc[cat_col])\n",
    "    X_test_ohe = pd.get_dummies(X_test_enc[cat_col])\n",
    "    missing_cols = set(X_train_ohe.columns ) - set(X_test_ohe.columns)\n",
    "    # Add a missing column in test set with default value equal to 0\n",
    "    for c in missing_cols:\n",
    "        X_test_ohe[c] = 0\n",
    "    # Ensure the order of column in the test set is in the same order than in train set\n",
    "    X_test_ohe = X_test_ohe[X_train_ohe.columns]\n",
    "\n",
    "    X_train_enc = pd.concat([X_train_enc, X_train_ohe], axis=1)\n",
    "    X_test_enc = pd.concat([X_test_enc, X_test_ohe], axis=1)\n",
    "\n",
    "    X_train_enc.drop(cat_col, axis=1, inplace=True)\n",
    "    X_test_enc.drop(cat_col, axis=1, inplace=True)\n",
    "    \n",
    "    X_train_cols = X_train_enc.columns\n",
    "    X_test_cols = X_test_enc.columns\n",
    "        \n",
    "    return X_train_enc, X_test_enc, X_train_cols, X_test_cols\n",
    "\n",
    "\n",
    "def add_interactions(X_train_enc, X_test_enc, bst, gain_rank_filter):\n",
    "\n",
    "      xgbfir.saveXgbFI(bst, MaxInteractionDepth=3, OutputXlsxFile=path+'XgbFeatureInteractions.xlsx')\n",
    "      xl_file = pd.ExcelFile(path+'XgbFeatureInteractions.xlsx')\n",
    "\n",
    "      dfs = {sheet_name: xl_file.parse(sheet_name) \n",
    "                for sheet_name in xl_file.sheet_names}\n",
    "\n",
    "      interactions = list(dfs['Interaction Depth 1'][dfs['Interaction Depth 1']['Gain Rank']<gain_rank_filter]['Interaction'].unique()) + \n",
    "                     list(dfs['Interaction Depth 2'][dfs['Interaction Depth 2']['Gain Rank']<gain_rank_filter]['Interaction'].unique()) + \n",
    "                     list(dfs['Interaction Depth 3'][dfs['Interaction Depth 3']['Gain Rank']<gain_rank_filter]['Interaction'].unique())\n",
    "\n",
    "      clean_interactions = []\n",
    "      i = 0\n",
    "\n",
    "      for var in interactions:\n",
    "          \n",
    "          var = list(set(var.split('|')))\n",
    "          if (len(var) > 1):\n",
    "            clean_interactions += [var]\n",
    "          i = i + 1\n",
    "\n",
    "      clean_interactions.sort()\n",
    "      clean_interactions = list(clean_interactions for clean_interactions,_ in itertools.groupby(clean_interactions))\n",
    "\n",
    "      for i in range(0, len(clean_interactions)):\n",
    "        if (len(clean_interactions[i])==2):\n",
    "            X_train_enc[clean_interactions[i][0]+'|'+clean_interactions[i][1]]=X_train_enc[clean_interactions[i][0]]*X_train_enc[clean_interactions[i][1]]\n",
    "            X_test_enc[clean_interactions[i][0]+'|'+clean_interactions[i][1]]=X_test_enc[clean_interactions[i][0]]*X_test_enc[clean_interactions[i][1]]\n",
    "        if (len(clean_interactions[i])==3):\n",
    "            X_train_enc[clean_interactions[i][0]+'|'+clean_interactions[i][1]+'|'+clean_interactions[i][2]]=X_train_enc[clean_interactions[i][0]]*X_train_enc[clean_interactions[i][1]]*X_train_enc[clean_interactions[i][2]]\n",
    "            X_test_enc[clean_interactions[i][0]+'|'+clean_interactions[i][1]+'|'+clean_interactions[i][2]]=X_test_enc[clean_interactions[i][0]]*X_test_enc[clean_interactions[i][1]]*X_test_enc[clean_interactions[i][2]]\n",
    "        if (len(clean_interactions[i])==4):\n",
    "            X_train_enc[clean_interactions[i][0]+'|'+clean_interactions[i][1]+'|'+clean_interactions[i][2]+'|'+clean_interactions[i][3]]=X_train_enc[clean_interactions[i][0]]*X_train_enc[clean_interactions[i][1]]*X_train_enc[clean_interactions[i][2]]*X_train_enc[clean_interactions[i][3]]\n",
    "            X_test_enc[clean_interactions[i][0]+'|'+clean_interactions[i][1]+'|'+clean_interactions[i][2]+'|'+clean_interactions[i][3]]=X_test_enc[clean_interactions[i][0]]*X_test_enc[clean_interactions[i][1]]*X_test_enc[clean_interactions[i][2]]*X_test_enc[clean_interactions[i][3]]\n",
    "      \n",
    "      X_train_cols, X_test_cols = X_train_enc.columns, X_test_enc.columns\n",
    "\n",
    "      return X_train_enc, X_test_enc, X_train_cols, X_test_cols\n",
    "  \n",
    "def draw_feature(selected_cols, max_depth, min_samples_leaf):\n",
    "        \n",
    "        clf = DecisionTreeRegressor(max_depth=max_depth, min_samples_leaf=min_samples_leaf, criterion='mae')\n",
    "        clf.fit(X_train_enc[selected_cols], Y_train)\n",
    "        \n",
    "        clf_tmp = DecisionTreeRegressor(max_depth=1, min_samples_leaf=min_samples_leaf, criterion = 'mae')\n",
    "        clf_tmp.fit(X_train_enc[selected_cols], Y_train)\n",
    "        top_feature = np.array(selected_cols)[clf_tmp.feature_importances_ > 0]\n",
    "        \n",
    "        dot_data = StringIO()\n",
    "        export_graphviz(clf, out_file=dot_data, feature_names = selected_cols,\n",
    "                        filled=True, rounded=True, proportion = True, impurity = False,\n",
    "                        special_characters=True,\n",
    "                        class_names = ['GOOD', 'BAD']\n",
    "                       )\n",
    "        graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "        graph.write_png('original_tree.png')\n",
    "        graph.set_size('\"75,!\"')\n",
    "        plt = Image(graph.create_png())\n",
    "        display(plt)\n",
    "        return top_feature\n",
    "    \n",
    "    \n",
    "def get_variables(coefficients, top = 10):\n",
    "    var_list = coefficients.sort_values(by = 'weight', ascending = False).head(top)['variable'].unique()\n",
    "\n",
    "    draw_interactions = []\n",
    "    i = 0\n",
    "\n",
    "    for var in var_list:\n",
    "        \n",
    "        var = list(set(var.split('|')))\n",
    "        draw_interactions += var\n",
    "        i = i + 1\n",
    "\n",
    "    draw_interactions.sort()\n",
    "    draw_interactions = list(set(draw_interactions))\n",
    "    return draw_interactions\n",
    "\n",
    "\n",
    "def get_variable(coefficients, num = 145):\n",
    "    var_list = [coefficients.ix[num]['variable']]\n",
    "\n",
    "    draw_interactions = []\n",
    "    i = 0\n",
    "\n",
    "    for var in var_list:\n",
    "        \n",
    "        var = list(set(var.split('|')))\n",
    "        draw_interactions += var\n",
    "        i = i + 1\n",
    "\n",
    "    draw_interactions.sort()\n",
    "    draw_interactions = list(set(draw_interactions))\n",
    "    return draw_interactions\n",
    "\n",
    "def xgb_mape(preds, dtrain):\n",
    "   labels = dtrain.get_label()\n",
    "   return('mape', -np.mean(np.abs((labels - preds) / (labels + 1))))\n",
    "\n",
    "\n",
    "class TimeBasedCV(object):\n",
    "    '''\n",
    "    Parameters \n",
    "    ----------\n",
    "    train_period: int\n",
    "        number of time units to include in each train set\n",
    "        default is 30\n",
    "    test_period: int\n",
    "        number of time units to include in each test set\n",
    "        default is 7\n",
    "    freq: string\n",
    "        frequency of input parameters. possible values are: days, months, years, weeks, hours, minutes, seconds\n",
    "        possible values designed to be used by dateutil.relativedelta class\n",
    "        deafault is days\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, train_period=30, test_period=7, freq='days'):\n",
    "        self.train_period = train_period\n",
    "        self.test_period = test_period\n",
    "        self.freq = freq\n",
    "\n",
    "        \n",
    "        \n",
    "    def split(self, data, validation_split_date=None, date_column='record_date', gap=0):\n",
    "        '''\n",
    "        Generate indices to split data into training and test set\n",
    "        \n",
    "        Parameters \n",
    "        ----------\n",
    "        data: pandas DataFrame\n",
    "            your data, contain one column for the record date \n",
    "        validation_split_date: datetime.date()\n",
    "            first date to perform the splitting on.\n",
    "            if not provided will set to be the minimum date in the data after the first training set\n",
    "        date_column: string, deafult='record_date'\n",
    "            date of each record\n",
    "        gap: int, default=0\n",
    "            for cases the test set does not come right after the train set,\n",
    "            *gap* days are left between train and test sets\n",
    "        \n",
    "        Returns \n",
    "        -------\n",
    "        train_index ,test_index: \n",
    "            list of tuples (train index, test index) similar to sklearn model selection\n",
    "        '''\n",
    "        \n",
    "        # check that date_column exist in the data:\n",
    "        try:\n",
    "            data[date_column]\n",
    "        except:\n",
    "            raise KeyError(date_column)\n",
    "                    \n",
    "        train_indices_list = []\n",
    "        test_indices_list = []\n",
    "\n",
    "        if validation_split_date==None:\n",
    "            validation_split_date = data[date_column].min().date() + eval('relativedelta('+self.freq+'=self.train_period)')\n",
    "        \n",
    "        start_train = validation_split_date - eval('relativedelta('+self.freq+'=self.train_period)')\n",
    "        end_train = start_train + eval('relativedelta('+self.freq+'=self.train_period)')\n",
    "        start_test = end_train + eval('relativedelta('+self.freq+'=gap)')\n",
    "        end_test = start_test + eval('relativedelta('+self.freq+'=self.test_period)')\n",
    "\n",
    "        while end_test < data[date_column].max().date():\n",
    "            # train indices:\n",
    "            cur_train_indices = list(data[(data[date_column].dt.date>=start_train) & \n",
    "                                     (data[date_column].dt.date<end_train)].index)\n",
    "\n",
    "            # test indices:\n",
    "            cur_test_indices = list(data[(data[date_column].dt.date>=start_test) &\n",
    "                                    (data[date_column].dt.date<end_test)].index)\n",
    "            \n",
    "            print(\"Train period:\",start_train,\"-\" , end_train, \", Test period\", start_test, \"-\", end_test,\n",
    "                  \"# train records\", len(cur_train_indices), \", # test records\", len(cur_test_indices))\n",
    "\n",
    "            train_indices_list.append(cur_train_indices)\n",
    "            test_indices_list.append(cur_test_indices)\n",
    "\n",
    "            # update dates:\n",
    "            start_train = start_train + eval('relativedelta('+self.freq+'=self.test_period)')\n",
    "            end_train = start_train + eval('relativedelta('+self.freq+'=self.train_period)')\n",
    "            start_test = end_train + eval('relativedelta('+self.freq+'=gap)')\n",
    "            end_test = start_test + eval('relativedelta('+self.freq+'=self.test_period)')\n",
    "\n",
    "        # mimic sklearn output  \n",
    "        index_output = [(train,test) for train,test in zip(train_indices_list,test_indices_list)]\n",
    "\n",
    "        self.n_splits = len(index_output)\n",
    "        \n",
    "        return index_output\n",
    "    \n",
    "    \n",
    "    def get_n_splits(self):\n",
    "        \"\"\"Returns the number of splitting iterations in the cross-validator\n",
    "        Returns\n",
    "        -------\n",
    "        n_splits : int\n",
    "            Returns the number of splitting iterations in the cross-validator.\n",
    "        \"\"\"\n",
    "        return self.n_splits \n",
    "    \n",
    "    \n",
    "def percentage_error(actual, predicted):\n",
    "    res = np.empty(actual.shape)\n",
    "    for j in range(actual.shape[0]):\n",
    "        if actual[j] != 0:\n",
    "            res[j] = (actual[j] - predicted[j]) / actual[j]\n",
    "        else:\n",
    "            res[j] = predicted[j] / np.mean(actual)\n",
    "    return res\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    return np.mean(np.abs(percentage_error(np.asarray(y_true), np.asarray(y_pred)))) \n",
    "\n",
    "def is_holiday(date, country='NL'):\n",
    "  nl_holidays = holidays.CountryHoliday('NL')\n",
    "  return int(date in nl_holidays)\n",
    "\n",
    "def is_3days_before_holiday(date, country='NL'):\n",
    "  nl_holidays = holidays.CountryHoliday('NL')\n",
    "  return int((date + pd.to_timedelta(1, unit=\"D\") in nl_holidays)|(date + pd.to_timedelta(2, unit=\"D\") in nl_holidays)|(date + pd.to_timedelta(3, unit=\"D\") in nl_holidays))\n",
    "\n",
    "def is_3days_after_holiday(date, country='NL'):\n",
    "  nl_holidays = holidays.CountryHoliday('NL')\n",
    "  return int((date - pd.to_timedelta(1, unit=\"D\") in nl_holidays)|(date - pd.to_timedelta(2, unit=\"D\") in nl_holidays)|(date - pd.to_timedelta(3, unit=\"D\") in nl_holidays))\n",
    "\n",
    "def is_6days_before_holiday(date, country='NL'):\n",
    "  nl_holidays = holidays.CountryHoliday('NL')\n",
    "  return int((date + pd.to_timedelta(1, unit=\"D\") in nl_holidays)|(date + pd.to_timedelta(2, unit=\"D\") in nl_holidays)|(date + pd.to_timedelta(3, unit=\"D\") in nl_holidays)|\n",
    "             (date + pd.to_timedelta(4, unit=\"D\") in nl_holidays)|(date + pd.to_timedelta(5, unit=\"D\") in nl_holidays)|(date + pd.to_timedelta(6, unit=\"D\") in nl_holidays))\n",
    "\n",
    "def is_6days_after_holiday(date, country='NL'):\n",
    "  nl_holidays = holidays.CountryHoliday('NL')\n",
    "  return int((date - pd.to_timedelta(1, unit=\"D\") in nl_holidays)|(date - pd.to_timedelta(2, unit=\"D\") in nl_holidays)|(date - pd.to_timedelta(3, unit=\"D\") in nl_holidays)|\n",
    "             (date - pd.to_timedelta(4, unit=\"D\") in nl_holidays)|(date - pd.to_timedelta(5, unit=\"D\") in nl_holidays)|(date - pd.to_timedelta(6, unit=\"D\") in nl_holidays))\n",
    "    \n",
    "def date_features(data, dt_col):\n",
    "      data = data.reset_index()\n",
    "      data[dt_col]=pd.to_datetime(data[dt_col], format = '%Y-%m-%d')\n",
    "      data['week'] = data[dt_col].dt.week\n",
    "      data['weekday'] = data[dt_col].dt.weekday\n",
    "      data['is_summer'] = ((data[dt_col].dt.month >= 5) & (data[dt_col].dt.month <= 9)).astype(int)\n",
    "      data['is_winter'] = ((data[dt_col].dt.month >= 10) & (data[dt_col].dt.month <= 4)).astype(int)\n",
    "      # data['is_month_start'] = data[dt_col].dt.is_month_start.astype(int)\n",
    "      # data['is_month_end'] = data[dt_col].dt.is_month_end.astype(int)\n",
    "      # data['is_month_middle'] = (data[dt_col].apply(lambda x: x + relativedelta(days=-15))).dt.is_month_start.astype(int)\n",
    "      \n",
    "      return data.set_index(dt_col), data.set_index(dt_col).columns\n",
    "    \n",
    "    \n",
    "def holiday_features(data, dt_col):\n",
    "      data = data.reset_index()\n",
    "      data[dt_col]=pd.to_datetime(data[dt_col], format = '%Y-%m-%d')\n",
    "      data['is_holiday'] = data[dt_col].dt.date.apply(is_holiday)\n",
    "      data['is_3days_before_holiday'] = data[dt_col].dt.date.apply(is_3days_before_holiday)\n",
    "      data['is_3days_after_holiday'] = data[dt_col].dt.date.apply(is_3days_after_holiday)\n",
    "      data['is_6days_before_holiday'] = data[dt_col].dt.date.apply(is_6days_before_holiday)\n",
    "      data['is_6days_after_holiday'] = data[dt_col].dt.date.apply(is_6days_after_holiday)\n",
    "      \n",
    "      return data.set_index(dt_col), data.set_index(dt_col).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Column Group - Mobility, Restrictions and Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_c = ['retail_and_recreation_percent_change_from_baseline_WEEK_-1',\n",
    "       'grocery_and_pharmacy_percent_change_from_baseline_WEEK_-1',\n",
    "       'parks_percent_change_from_baseline_WEEK_-1',\n",
    "       'transit_stations_percent_change_from_baseline_WEEK_-1',\n",
    "       'workplaces_percent_change_from_baseline_WEEK_-1',\n",
    "       'residential_percent_change_from_baseline_WEEK_-1',\n",
    "       'retail_and_recreation_percent_change_from_baseline_WEEK_-2',\n",
    "       'grocery_and_pharmacy_percent_change_from_baseline_WEEK_-2',\n",
    "       'parks_percent_change_from_baseline_WEEK_-2',\n",
    "       'transit_stations_percent_change_from_baseline_WEEK_-2',\n",
    "       'workplaces_percent_change_from_baseline_WEEK_-2',\n",
    "       'residential_percent_change_from_baseline_WEEK_-2']\n",
    "\n",
    "cols_r =  ['AdaptationOfWorkplace_eq_1', 'ClosDaycare_eq_1',\n",
    "       'ClosDaycarePartial_eq_1', 'ClosHigh_eq_1', 'ClosHighPartial_eq_1',\n",
    "       'ClosPrim_eq_1', 'ClosPrimPartial_eq_1', 'ClosPubAnyPartial_eq_1',\n",
    "       'ClosSec_eq_1', 'ClosSecPartial_eq_1',\n",
    "       'ClosureOfPublicTransportPartial_eq_1', 'EntertainmentVenues_eq_1',\n",
    "       'EntertainmentVenuesPartial_eq_1', 'GymsSportsCentres_eq_1',\n",
    "       'GymsSportsCentresPartial_eq_1', 'IndoorOver100_eq_1',\n",
    "       'IndoorOver100_eq_3', 'IndoorOver5_eq_1', 'IndoorOver50_eq_1',\n",
    "       'IndoorOver50_eq_3', 'MasksMandatoryClosedSpaces_eq_1',\n",
    "       'MasksVoluntaryClosedSpaces_eq_1', 'MassGatherAll_eq_1',\n",
    "       'MassGatherAll_eq_3', 'MassGatherAllPartial_eq_1',\n",
    "       'OutdoorOver100_eq_1', 'OutdoorOver5_eq_1', 'OutdoorOver50_eq_1',\n",
    "       'OutdoorOver500_eq_1', 'PlaceOfWorship_eq_1',\n",
    "       'PlaceOfWorshipPartial_eq_1', 'RestaurantsCafes_eq_1',\n",
    "       'RestaurantsCafesPartial_eq_1', 'RestaurantsCafesPartial_eq_3',\n",
    "       'StayHomeGen_eq_1', 'StayHomeGenPartial_eq_1', 'StayHomeRiskG_eq_1',\n",
    "       'Teleworking_eq_1', 'WorkplaceClosures_eq_1']\n",
    "\n",
    "cols_e = [\n",
    " 'categories.community',\n",
    " 'categories.concerts',\n",
    " 'categories.public-holidays',\n",
    " 'categories.school-holidays',\n",
    " 'categories.sports',\n",
    " 'categories_impact.community',\n",
    " 'categories_impact.concerts',\n",
    " 'categories_impact.conferences',\n",
    " 'categories_impact.public-holidays',\n",
    " 'categories_impact.school-holidays',\n",
    " 'categories_impact.sports']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Pipeline Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'UNDER_ESTIMATION'\n",
    "index = 'validdate'\n",
    "# test_size = 0.2\n",
    "nrounds_xgb = 600\n",
    "nfolds_xgb = 6\n",
    "early_stopping_rounds = 30\n",
    "gain_rank_filter = 60\n",
    "param_grid = {'max_depth':[1,2,3], 'min_samples_leaf':[55], 'criterion':['mae']}\n",
    "n_folds = 4\n",
    "validation_split_date = datetime.date(2020,4,16)\n",
    "test_period = 1\n",
    "bins = 6 \n",
    "min_trend_corr = 0\n",
    "max_trend_chng = 4\n",
    "# alpha = 1\n",
    "rule_to_save = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Used Features, Train and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_features = ['LOCATION_ID', 'PCT_SHARE_OFFER_SALES','wind_dir_10m:d', 'wind_speed_10m:ms', \n",
    "                't_2m:C', 'relative_humidity_2m:p', 'sfc_pressure:Pa', 't_apparent:C',\n",
    "                'windchill:C', 'heat_index:C', 'leisure_beach:idx',\n",
    "                'effective_cloud_cover:p', #  'tropical_nights:d',\n",
    "                'growing_degree_days_accumulated:gdd', 'hot_days:d',\n",
    "                'high_cloud_cover:p', 'leaf_wetness:idx', 'global_rad_12h:J', 'wind_gusts_10m_12h:ms',\n",
    "                'precip_12h:mm', 'weather_symbol_12h:idx','snow_warning_12h:idx', 'clear_sky_energy_12h:J',\n",
    "                'evapotranspiration_12h:mm', 'sunshine_duration_12h:min',\n",
    "                'evaporation_12h:mm', 't_max_2m_12h:C', 't_min_2m_12h:C'] + cols_e + cols_c\n",
    "\n",
    "train['LOCATION_ID']=train['LOCATION_ID'].astype('str')\n",
    "\n",
    "X = train.set_index(index)[use_features]\n",
    "X_test = test.set_index(index)[use_features]\n",
    "Y = -train.set_index(index)[target]\n",
    "Yd = np.log1p(train.set_index(index)['SALES'])-np.log1p(train.set_index(index)['MODEL']).reindex(Y.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine Boolean, Numeric and Category Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_col = list(X.select_dtypes(include=['bool']).columns)\n",
    "numeric_col = list(set(X.select_dtypes(include=['float', 'int']).columns))\n",
    "cat_col = X.select_dtypes(include=['object']).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data for Train and Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_old = X.columns\n",
    "X, cols = date_features(X, index)\n",
    "X, cols = holiday_features(X, index)\n",
    "use_features = cols\n",
    "\n",
    "tscv = TimeBasedCV(train_period=(validation_split_date - np.min(X.index).date()).days,\n",
    "                   test_period=(np.max(X.index).date()-validation_split_date).days-1,\n",
    "                   freq='days')\n",
    "X = X.reset_index()\n",
    "Y = Y.reset_index()\n",
    "Yd = Yd.reset_index()\n",
    "\n",
    "for train_index, test_index in tqdm(tscv.split(X.reset_index(), validation_split_date=validation_split_date, date_column = index)):\n",
    "              X_train = X.iloc[train_index].set_index([index])\n",
    "              Y_train = Y.iloc[train_index].set_index([index])\n",
    "              X_valid = X.iloc[test_index].set_index([index])\n",
    "              Y_valid = Y.iloc[test_index].set_index([index])\n",
    "              Yd_train = Yd.iloc[train_index].set_index([index])\n",
    "              Yd_valid = Yd.iloc[test_index].set_index([index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Noisy Train/Test Variables by Trend Correlation / Trend Changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xf_train = X_train\n",
    "Xf_train['UNDER_ESTIMATION'] = Y_train\n",
    "Xf_valid = X_valid\n",
    "Xf_valid['UNDER_ESTIMATION'] = Y_valid\n",
    "\n",
    "feats = list(set(use_features[1:]))\n",
    "stats = get_trend_stats(data=Xf_train, target_col='UNDER_ESTIMATION', data_test=Xf_valid, features_list=feats)\n",
    "\n",
    "trend = {}\n",
    "for feat in feats:\n",
    "    f = get_grouped_data(Xf_train, feat, 'UNDER_ESTIMATION', bins, cuts=0)[1]\n",
    "    y = f['UNDER_ESTIMATION_mean']\n",
    "    x = f[feat + '_mean']\n",
    "    trend[feat] = np.sign(linregress(x, y)[0])\n",
    "\n",
    "stats['mono'] = stats['Feature'].map(trend)\n",
    "stats = stats[(stats['Trend_correlation']>min_trend_corr) & (stats['Trend_changes_test']-stats['Trend_changes'] < max_trend_chng)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Numeric Features with Tree and then encode all features with One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_scorer = make_scorer(mape, greater_is_better=False)\n",
    "X_train_enc, X_valid_enc= encode_float_tree(X_train, X_valid, Yd_train, use_features, numeric_col, target, n_folds, param_grid, mape_scorer)\n",
    "\n",
    "cat_col = list(X_train_enc.select_dtypes(include=['object']).columns) \n",
    "X_train_enc, X_valid_enc, X_train_cols, X_valid_cols = encode_data_ohe(X_train_enc, X_valid_enc, Y_train, use_features, bool_col, cat_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exclude Filtered Variables from XGBoost Feature List\n",
    "... and calculate monotonicity values behind the scenes ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_features = list(set(stats['Feature']))\n",
    "good_mono = list(stats['mono']) \n",
    "xgb_features = []\n",
    "feature_monotones = []\n",
    "\n",
    "for f1 in tqdm(X_train_enc.columns):\n",
    "    for ix, f2 in enumerate(good_features):\n",
    "       if (f2 in f1):\n",
    "           xgb_features.append(f1)\n",
    "           feature_monotones.append(int(good_mono[ix]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train First Level XGBoost Model with CV (Optimizing MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_train_enc[xgb_features], label = Yd_train.clip(0))\n",
    "dtest =  xgb.DMatrix(X_valid_enc[xgb_features], label = Yd_valid.clip(0))\n",
    "\n",
    "params = {'max_depth': 6,\n",
    "          'eta': 0.1,\n",
    "          'gamma': 10,\n",
    "          'silent': 1,\n",
    "          'nthread': 1,\n",
    "          'min_child_weight': 25,\n",
    "          'seed': 42,\n",
    "          'tree_method': 'hist',\n",
    "          'grow_policy': 'lossguide',\n",
    "          'eval_metric': 'mae',\n",
    "          'objective': 'reg:gamma',\n",
    "          'monotone_constraints': '(' + ','.join([str(m) for m in feature_monotones]) + ')'\n",
    "         }\n",
    "\n",
    "# Use CV to find the best number of trees\n",
    "bst_cv = xgb.cv(params, dtrain, nrounds_xgb, seed=42, nfold = nfolds_xgb, \n",
    "                early_stopping_rounds=early_stopping_rounds, feval = xgb_mape, maximize=True, verbose_eval=True)\n",
    "\n",
    "evallist  = [(dtrain, 'train'), (dtest, 'eval')]\n",
    "evals_result = {}\n",
    "bst = xgb.train(params, dtrain, num_boost_round = bst_cv.shape[0], evals_result = evals_result, \n",
    "                evals = evallist,  verbose_eval = True, feval = xgb_mape, maximize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Interactions to Train and Valid Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_enc, X_valid_enc, X_train_cols, X_valid_cols = add_interactions(X_train_enc, X_valid_enc, bst, \n",
    "                                                                        gain_rank_filter = gain_rank_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validate with Second Level model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "X_train_enc_d = pd.concat((X_train_enc, X_valid_enc), axis=0).reset_index()\n",
    "Y_train_d =pd.concat((Y_train, Y_valid), axis=0)\n",
    "X_train_enc_d[index]=pd.to_datetime(X_train_enc_d[index], format = '%Y-%m-%d')\n",
    "loc_cols = list(train['LOCATION_ID'].unique())\n",
    "\n",
    "tscv = TimeBasedCV(train_period=(validation_split_date - np.min(X_train_enc_d[index]).date()).days,\n",
    "                   test_period=test_period,\n",
    "                   freq='days')\n",
    "\n",
    "scores_mape = []\n",
    "scores_mae = []\n",
    "scores_rmsle = []\n",
    "pred_act = pd.DataFrame()\n",
    "\n",
    "for train_index, test_index in tqdm(tscv.split(X_train_enc_d, validation_split_date=validation_split_date, date_column = index)):\n",
    "\n",
    "    data_train   = X_train_enc_d.loc[train_index].drop(index, axis=1)\n",
    "    target_train = Y_train_d.iloc[train_index]\n",
    "\n",
    "    data_test    = X_train_enc_d.loc[test_index].drop(index, axis=1)\n",
    "    target_test  = Y_train_d.iloc[test_index]\n",
    "\n",
    "    # clf = ElasticNet(alpha=0.0001)\n",
    "    clf = BayesianRidge()\n",
    "    clf.fit(data_train[X_train_cols],target_train)\n",
    "\n",
    "    preds, err = clf.predict(data_test, return_std=True)\n",
    "    \n",
    "    last_ind = 0\n",
    "    for idx, loc in enumerate(loc_cols):\n",
    "          \n",
    "          preds_loc = preds[last_ind + test_period*idx:last_ind + test_period*(idx+1)]\n",
    "          err_loc = err[last_ind + test_period*idx:last_ind + test_period*(idx+1)]\n",
    "          target_test_loc = target_test[last_ind + test_period*idx:last_ind + test_period*(idx+1)]\n",
    "          test_index_loc = test_index[last_ind + test_period*idx:last_ind + test_period*(idx+1)]\n",
    "\n",
    "          last_ind+= test_period\n",
    "          pred = pd.DataFrame(np.round(preds_loc).clip(0), columns = ['value'])\n",
    "          pred['low']=np.round(preds_loc).clip(0) - err_loc.values\n",
    "          pred['upp']=np.round(preds_loc).clip(0) + err_loc.values\n",
    "          pred['validdate']=X_train_enc_d.loc[test_index_loc][index].values\n",
    "          pred['metric']='Forecast'\n",
    "          pred['location_id']=loc\n",
    "          act = pd.DataFrame(target_test_loc.values, columns = ['value'])\n",
    "          act['validdate']=X_train_enc_d.loc[test_index_loc][index].values\n",
    "          act['metric']='Actual'\n",
    "          act['location_id']=loc\n",
    "          # accuracy for the current fold only\n",
    "          if pred_act.empty:\n",
    "                  pred_act = pd.concat((pred, act), axis = 0)\n",
    "          else:\n",
    "                  batch = pd.concat((pred, act), axis = 0)\n",
    "                  pred_act = pd.concat((pred_act, batch), axis = 0)\n",
    "          \n",
    "          last_ind = 0\n",
    "\n",
    "    score_mape = mean_absolute_percentage_error(target_test.values,np.round(preds).clip(0))\n",
    "    score_mae = mean_absolute_error(target_test.values,np.round(preds).clip(0))\n",
    "    score_rmsle = np.sqrt(mean_squared_log_error(target_test.values,np.round(preds).clip(0)))\n",
    "\n",
    "    scores_mape.append(score_mape)\n",
    "    scores_mae.append(np.log1p(score_mae))\n",
    "    scores_rmsle.append(score_rmsle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Metrics Dynamics - LogMAE, MAPE, RMSLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days=((np.max(X_train_enc_d[index]).date()-validation_split_date).days//test_period)*test_period - 1\n",
    "\n",
    "scores_mape = pd.DataFrame(np.array(scores_mape), columns = ['value'])\n",
    "scores_mape['metric']='MAPE'\n",
    "scores_mape['date']=pd.date_range(start=validation_split_date + relativedelta(days=+1),end=validation_split_date + relativedelta(days=+days), freq = str(test_period)+'D')\n",
    "\n",
    "scores_mae= pd.DataFrame(np.array(scores_mae), columns = ['value'])\n",
    "scores_mae['metric']='LogMAE'\n",
    "scores_mae['date']=pd.date_range(start=validation_split_date + relativedelta(days=+1),end=validation_split_date + relativedelta(days=+days), freq = str(test_period)+'D')\n",
    "\n",
    "scores_rmsle= pd.DataFrame(np.array(scores_rmsle), columns = ['value'])\n",
    "scores_rmsle['metric']='RMSLE'\n",
    "scores_rmsle['date']=pd.date_range(start=validation_split_date + relativedelta(days=+1),end=validation_split_date + relativedelta(days=+days), freq = str(test_period)+'D')\n",
    "\n",
    "scores = pd.concat((scores_mape, scores_mae), axis=0)\n",
    "scores = pd.concat((scores, scores_rmsle), axis=0)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(18, 8.27)\n",
    "sns.lineplot(data=scores, x = 'date', y='value', hue='metric')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refit Model with Parameters define on the whole Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import BayesianRidge\n",
    "# clf1 = ElasticNet(alpha=0.0001)\n",
    "clf1 = BayesianRidge()\n",
    "clf1.fit(X_train_enc, Y_train)\n",
    "coefs1 = clf1.coef_/np.sum(clf1.coef_)\n",
    "\n",
    "pred_tr, err_tr = clf1.predict(X_train_enc, return_std=True)\n",
    "pred_te, err_te = clf1.predict(X_valid_enc, return_std=True)\n",
    "pred_tr = pred_tr.clip(0)\n",
    "pred_te = pred_te.clip(0)\n",
    "\n",
    "coefs=clf1.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Model Validation Performance and Threshold Approximation Strategy\n",
    "\n",
    "RMSE leads to estimating mean and is more sensitive to extreme cases and outliers. MAE leads to estimating median and is more robust to outliers. If RMSE dropped but MAE increased, it means that your model is now better at accounting for extreme cases, but the solution may be less robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MAPE Train: %.4f\" % mean_absolute_percentage_error(Y_train, pred_tr))\n",
    "print(\"LogMAE Train: %.4f\" % np.log1p(mean_absolute_error(Y_train, pred_tr)))\n",
    "print(\"RMSLE Train: %.4f\" % np.sqrt(mean_squared_log_error(Y_train, pred_tr)))\n",
    "\n",
    "print(\"MAPE Test: %.4f\" % mean_absolute_percentage_error(Y_valid, pred_te))\n",
    "print(\"LogMAE Test: %.4f\" % np.log1p(mean_absolute_error(Y_valid, pred_te)))\n",
    "print(\"RMSLE Test: %.4f\" % np.sqrt(mean_squared_log_error(Y_valid, pred_te)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Forecast vs Actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_ind = 0\n",
    "df_tr = pd.DataFrame()\n",
    "for idx, loc in enumerate(loc_cols):\n",
    "          filter = X_train_enc['LOCATION_ID_'+str(loc)]==1 \n",
    "          pred_tr_loc = pred_tr[filter]\n",
    "          Y_train_loc = Y_train[filter]\n",
    "          X_train_enc_loc = X_train_enc[filter]\n",
    "          err_tr_loc = err_tr[filter]\n",
    "\n",
    "          pre_tr = pd.DataFrame()\n",
    "          pre_tr['value']=pred_tr_loc\n",
    "          pre_tr['metric']='Forecast'\n",
    "          pre_tr['location_id']=loc\n",
    "          pre_tr.index = pd.to_datetime(X_train_enc_loc.index, format = '%Y-%m-%d')\n",
    "          pre_tr['low']=pre_tr['value']- err_tr_loc\n",
    "          pre_tr['upp']=pre_tr['value']+ err_tr_loc\n",
    "\n",
    "          act_tr = pd.DataFrame()\n",
    "          act_tr ['value']=Y_train_loc.values.squeeze()\n",
    "          act_tr ['metric']='Actual'\n",
    "          act_tr['location_id']=loc\n",
    "          act_tr.index = pd.to_datetime(X_train_enc_loc.index, format = '%Y-%m-%d')\n",
    "\n",
    "          if df_tr.empty:\n",
    "                  df_tr = pd.concat((pre_tr, act_tr), axis=0).reset_index()\n",
    "          else:\n",
    "                  batch = pd.concat((pre_tr, act_tr), axis=0).reset_index()\n",
    "\n",
    "                  df_tr = pd.concat((df_tr, batch), axis = 0)\n",
    "          \n",
    "          last_ind = 0 \n",
    "\n",
    "df = pd.concat((pred_act, df_tr), axis=0)\n",
    "\n",
    "plotting = pred_act.groupby(['location_id']).agg(lambda x: ','.join(x))\n",
    "for group in plotting.index:\n",
    "    fig = plt.figure(figsize=(50,10))\n",
    "    \n",
    "    ax = fig.add_subplot(221)\n",
    "    fig.set_size_inches(50, 18)\n",
    "    \n",
    "    df_p = df[(df['location_id']==group) & (df['validdate'].dt.date >=datetime.date(2020,3,10))].sort_values(by='validdate', ascending=True)\n",
    "    idx = df_p[df_p['metric']=='Forecast']['validdate']\n",
    "    low = df_p[df_p['metric']=='Forecast']['low']\n",
    "    upp = df_p[df_p['metric']=='Forecast']['upp']\n",
    "    \n",
    "    ax = sns.lineplot(data = df_p, x='validdate', y='value', hue='metric')\n",
    "    ax.axvline((validation_split_date), label='validation_start', color='r', linestyle='--', lw=2)\n",
    "    plt.fill_between(idx, low, upp, alpha=.3)\n",
    "    plt.title('Forecast vs Actual '+ str(group))\n",
    "    plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Model Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 1000)\n",
    "coefficients = pd.concat([pd.DataFrame(X_train_enc.columns),pd.DataFrame(np.transpose(coefs))], axis = 1)\n",
    "coefficients.columns = ['variable', 'weight']\n",
    "coefficients['percent']=np.abs(coefficients['weight'])/np.sum(np.abs(coefficients['weight']))\n",
    "coefficients.sort_values(by = 'weight', ascending = False).head(70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Feature Importance Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(20, 15))\n",
    "splot = sns.barplot(x=\"percent\", y=\"variable\", data=coefficients.sort_values(by=\"percent\", ascending=False).head(50))\n",
    "\n",
    "plt.title('Feature Importance')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Interesting Rulesets (RFE-like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_interactions = get_variables(coefficients, top=100)\n",
    "i = 0\n",
    "while len(draw_interactions) > 20:\n",
    "      i += 1\n",
    "      print('Ruleset Num. ' + str(i))\n",
    "      print('')\n",
    "\n",
    "      top_feature = draw_feature(draw_interactions, max_depth = 2, min_samples_leaf = 10)\n",
    "      draw_interactions = list(set(draw_interactions)-set([top_feature[0]]))\n",
    "      # print(draw_interactions)\n",
    "      print('')\n",
    "      print(200*'=')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refit the Whole Model for Unknown Test Based of Full Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_features = ['LOCATION_ID', 'PCT_SHARE_OFFER_SALES','wind_dir_10m:d', 'wind_speed_10m:ms', \n",
    "                't_2m:C', 'relative_humidity_2m:p', 'sfc_pressure:Pa', 't_apparent:C',\n",
    "                'windchill:C', 'heat_index:C', 'leisure_beach:idx',\n",
    "                'effective_cloud_cover:p', #  'tropical_nights:d',\n",
    "                'growing_degree_days_accumulated:gdd', 'hot_days:d',\n",
    "                'high_cloud_cover:p', 'leaf_wetness:idx', 'global_rad_12h:J', 'wind_gusts_10m_12h:ms',\n",
    "                'precip_12h:mm', 'weather_symbol_12h:idx','snow_warning_12h:idx', 'clear_sky_energy_12h:J',\n",
    "                'evapotranspiration_12h:mm', 'sunshine_duration_12h:min',\n",
    "                'evaporation_12h:mm', 't_max_2m_12h:C', 't_min_2m_12h:C'] + cols_c + cols_e\n",
    "\n",
    "train['LOCATION_ID']=train['LOCATION_ID'].astype('str')\n",
    "\n",
    "X = train.set_index(index)[use_features]\n",
    "X_test = test.set_index(index)[use_features]\n",
    "Y = -train.set_index(index)[target]\n",
    "Yd = np.log1p(train.set_index(index)['SALES'])-np.log1p(train.set_index(index)['MODEL']).reindex(Y.index)\n",
    "\n",
    "cols_old = X.columns\n",
    "X, cols = date_features(X, index)\n",
    "X, cols = holiday_features(X, index)\n",
    "X_test, cols = date_features(X_test, index)\n",
    "X_test, cols = holiday_features(X_test, index)\n",
    "use_features+= list(set(cols)-set(cols_old))\n",
    "\n",
    "mape_scorer = make_scorer(mape, greater_is_better=False)\n",
    "X_enc, X_test_enc= encode_float_tree(X, X_test.fillna(0), Yd, use_features, numeric_col, target, n_folds, param_grid, mape_scorer)\n",
    "\n",
    "cat_col = list(X_enc.select_dtypes(include=['object']).columns) \n",
    "X_enc, X_test_enc, X_cols, X_test_cols = encode_data_ohe(X_enc, X_test_enc, Y, use_features, bool_col, cat_col)\n",
    "\n",
    "xgb_features = []\n",
    "feature_monotones = []\n",
    "\n",
    "for f1 in tqdm(X_enc.columns):\n",
    "    for ix, f2 in enumerate(good_features):\n",
    "       if (f2 in f1):\n",
    "           xgb_features.append(f1)\n",
    "           feature_monotones.append(int(good_mono[ix]))\n",
    "\n",
    "dtrain = xgb.DMatrix(X_enc[xgb_features], label = Yd.clip(0))\n",
    "\n",
    "params = {'max_depth': 6,\n",
    "          'eta': 0.1,\n",
    "          # 'alpha': 0.1,\n",
    "          'gamma': 10,\n",
    "          'silent': 1,\n",
    "          'nthread': 1,\n",
    "          'min_child_weight': 25,\n",
    "          'seed': 42,\n",
    "          'tree_method': 'hist',\n",
    "          'grow_policy': 'lossguide',\n",
    "          'eval_metric': 'mae',\n",
    "          'objective': 'reg:gamma',\n",
    "          # E.g. fitting three features with positive, negative and no constraint\n",
    "          # 'monotone_constraints': (1,-1,0)\n",
    "          'monotone_constraints': '(' + ','.join([str(m) for m in feature_monotones]) + ')'\n",
    "         }\n",
    "\n",
    "bst = xgb.train(params, dtrain, num_boost_round = bst_cv.shape[0])\n",
    "\n",
    "X_enc, X_test_enc, X_cols, X_test_cols = add_interactions(X_enc, X_test_enc, bst, gain_rank_filter = gain_rank_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the Bayesian Linear Regression for the Second Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "clf1 = BayesianRidge()\n",
    "clf1.fit(X_enc, Y)\n",
    "coefs1 = clf1.coef_/np.sum(clf1.coef_)\n",
    "\n",
    "pred_tr, err_tr = clf1.predict(X_enc, return_std=True)\n",
    "pred_te, err_te = clf1.predict(X_test_enc, return_std=True)\n",
    "pred_tr = pred_tr.clip(0)\n",
    "pred_te = pred_te.clip(0)\n",
    "\n",
    "coefs=clf1.coef_\n",
    "coefficients = pd.concat([pd.DataFrame(X_enc.columns),pd.DataFrame(np.transpose(coefs))], axis = 1)\n",
    "coefficients.columns = ['variable', 'weight']\n",
    "coefficients['percent']=np.abs(coefficients['weight'])/np.sum(np.abs(coefficients['weight']))\n",
    "coefs = coefficients[np.round(coefficients['percent'],2)>0].sort_values(by='percent')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('timeseries_exploring')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "870bcbc9b573919602b6a76a542966a582d955027a13ceb2a724f81397026ae7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
