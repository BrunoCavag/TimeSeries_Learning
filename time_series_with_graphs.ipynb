{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Graphs and GNN to model time series"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Forecasting with Graph Convolutional Neural Networks\n",
    "\n",
    "Time Series forecasting tasks can be carried out following different approaches. The most classical is based on statistical and autoregressive methods. More tricky are the algorithms based on boosting and ensemble where we have to produce a good amount of useful handmade features with rolling periods. On the other side, we can find neural network models that enable more freedom in their development, providing customizable adoption of sequential modeling and much more.\n",
    "\n",
    "Recurrent and convolutional structure achieve great success in time series forecasting. Interesting approaches in the field are given by the adoption of Transformers and Attention architectures, originally native in the NLP. Uncommon seems to be the usage of graph structures, where we have a network composed of different nodes that are related by some kind of linkage to each other. What we try to do is to use a graphical representation of our time series to produce future forecasts.\n",
    "\n",
    "In this post, we carry out a sales forecasting task where we make use of graph convolutional neural networks exploiting the nested structure of our data, composed of different sales series of various items in different stores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import date, timedelta\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "### IMPORT SPEKTRAL CLASSES ###\n",
    "\n",
    "from spektral_utilities import *\n",
    "from spektral_gcn import GraphConv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data\n",
    "\n",
    "The dataset is collected form a padast competition on Kaggle. The store item demand forecasting challenge, provides 4 whole years of sales data in a daily format for different items sold in various stores. We have 10 stores and 50 products, for a total of 500 series. Each product is sold in every store. Our scope is to provide accurate future forecast daily for all the items.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### READ DATA ###\n",
    "\n",
    "df = pd.read_csv('sales_train.csv.zip')\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SWITCH DATA FROM VERTICAL TO HORIZONTAL FORMAT ###\n",
    "\n",
    "unstaked_df = df.copy()\n",
    "unstaked_df['id'] = df['item'].astype(str)+'_'+df['store'].astype(str)\n",
    "unstaked_df.set_index(['id','date'], inplace=True)\n",
    "unstaked_df.drop(['store','item'], axis=1, inplace=True)\n",
    "unstaked_df = unstaked_df.astype(float).unstack()\n",
    "unstaked_df.columns = unstaked_df.columns.get_level_values(1)\n",
    "\n",
    "print(unstaked_df.shape)\n",
    "unstaked_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### UTILITY FUNCTIONS FOR FEATURE ENGINEERING ###\n",
    "\n",
    "sequence_length = 14\n",
    "\n",
    "\n",
    "\n",
    "def get_timespan(df, today, days):    \n",
    "    df = df[pd.date_range(today - timedelta(days=days), \n",
    "            periods=days, freq='D')] # day - n_days <= dates < day    \n",
    "    return df\n",
    "\n",
    "def create_features(df, today, seq_len):\n",
    "    \n",
    "    all_sequence = get_timespan(df, today, seq_len).values\n",
    "    \n",
    "    group_store = all_sequence.reshape((-1, 10, seq_len))\n",
    "    \n",
    "    store_corr = np.stack([np.corrcoef(i) for i in group_store], axis=0)\n",
    "    \n",
    "    store_features = np.stack([\n",
    "              group_store.mean(axis=2),\n",
    "              group_store[:,:,int(sequence_length/2):].mean(axis=2),\n",
    "              group_store.std(axis=2),\n",
    "              group_store[:,:,int(sequence_length/2):].std(axis=2),\n",
    "              skew(group_store, axis=2),\n",
    "              kurtosis(group_store, axis=2),\n",
    "              np.apply_along_axis(lambda x: np.polyfit(np.arange(0, sequence_length), x, 1)[0], 2, group_store)\n",
    "            ], axis=1)\n",
    "    \n",
    "    group_store = np.transpose(group_store, (0,2,1))\n",
    "    store_features = np.transpose(store_features, (0,2,1))\n",
    "    \n",
    "    return group_store, store_corr, store_features\n",
    "\n",
    "def create_label(df, today):\n",
    "    \n",
    "    y = df[today].values\n",
    "    \n",
    "    return y.reshape((-1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PLOT A SEQUENCE OF SALES FOR ITEM 10 IN ALL STORES ###\n",
    "\n",
    "sequence = get_timespan(unstaked_df, date(2017,11,1), 30)\n",
    "sequence.head(10).T.plot(figsize=(14,5))\n",
    "plt.ylabel('sales')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINE TRAIN, VALID, TEST DATES ###\n",
    "\n",
    "train_date = date(2013, 1, 1)\n",
    "valid_date = date(2015, 1, 1)\n",
    "test_date = date(2016, 1, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data at our disposal is minimal: only sales amount and numerical encoding of items and stores. This is still enough for us to underline a basic hierarchical structure. All we need to do is to group the series at item levels, in this way we end with 50 groups (items) each composed by 10 series (items sold in each store); an example of a group is depicted in the figure above.\n",
    "\n",
    "In classical graph networks, all the relevant information is stored in an object called the adjacent matrix. This is a numerical representation of all the linkages present in the data. The adjacent matrix in our context can be retrieved by the correlation matrix calculated on sale sequences of a given item in all stores.\n",
    "\n",
    "The sequence repartition is fundamental in our approach because we decide to process the data in pieces like for recurrent architecture, which will be also part of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CREATE VALID FEATURES ###\n",
    "\n",
    "X_seq, X_cor, X_feat, y = [], [], [], []\n",
    "\n",
    "for d in tqdm(pd.date_range(valid_date+timedelta(days=sequence_length), test_date)):\n",
    "    seq_, corr_, feat_ = create_features(unstaked_df, d, sequence_length)\n",
    "    y_ = create_label(unstaked_df, d)\n",
    "    X_seq.append(seq_), X_cor.append(corr_), X_feat.append(feat_), y.append(y_)\n",
    "    \n",
    "X_valid_seq = np.concatenate(X_seq, axis=0).astype('float16')\n",
    "X_valid_cor = np.concatenate(X_cor, axis=0).astype('float16')\n",
    "X_valid_feat = np.concatenate(X_feat, axis=0).astype('float16')\n",
    "y_valid = np.concatenate(y, axis=0).astype('float16')\n",
    "\n",
    "print(X_valid_seq.shape, X_valid_cor.shape, X_valid_feat.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CREATE TEST FEATURES ###\n",
    "\n",
    "X_seq, X_cor, X_feat, y = [], [], [], []\n",
    "\n",
    "for d in tqdm(pd.date_range(test_date+timedelta(days=sequence_length), date(2016,12,31))):\n",
    "    seq_, corr_, feat_ = create_features(unstaked_df, d, sequence_length)\n",
    "    y_ = create_label(unstaked_df, d)\n",
    "    X_seq.append(seq_), X_cor.append(corr_), X_feat.append(feat_), y.append(y_)\n",
    "    \n",
    "X_test_seq = np.concatenate(X_seq, axis=0).astype('float16')\n",
    "X_test_cor = np.concatenate(X_cor, axis=0).astype('float16')\n",
    "X_test_feat = np.concatenate(X_feat, axis=0).astype('float16')\n",
    "y_test = np.concatenate(y, axis=0).astype('float16')\n",
    "\n",
    "print(X_test_seq.shape, X_test_cor.shape, X_test_feat.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SCALE SEQUENCES ###\n",
    "\n",
    "scaler_seq = StandardScaler()\n",
    "scaler_feat = StandardScaler()\n",
    "\n",
    "X_train_seq = scaler_seq.fit_transform(X_train_seq.reshape(-1,10)).reshape(X_train_seq.shape)\n",
    "X_valid_seq = scaler_seq.transform(X_valid_seq.reshape(-1,10)).reshape(X_valid_seq.shape)\n",
    "X_test_seq = scaler_seq.transform(X_test_seq.reshape(-1,10)).reshape(X_test_seq.shape)\n",
    "\n",
    "y_train = scaler_seq.transform(y_train)\n",
    "y_valid = scaler_seq.transform(y_valid)\n",
    "y_test = scaler_seq.transform(y_test)\n",
    "\n",
    "X_train_feat = scaler_feat.fit_transform(X_train_feat.reshape(-1,10)).reshape(X_train_feat.shape)\n",
    "X_valid_feat = scaler_feat.transform(X_valid_feat.reshape(-1,10)).reshape(X_valid_feat.shape)\n",
    "X_test_feat = scaler_feat.transform(X_test_feat.reshape(-1,10)).reshape(X_test_feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OBTAIN LAPLACIANS FROM CORRELATIONS ###\n",
    "\n",
    "X_train_lap = localpooling_filter(1 - np.abs(X_train_cor))\n",
    "X_valid_lap = localpooling_filter(1 - np.abs(X_valid_cor))\n",
    "X_test_lap = localpooling_filter(1 - np.abs(X_test_cor))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model\n",
    "\n",
    "Our model receives, as input, sequences of sales from all stores and adjacent matrixes obtained from the same sequences. The sequences are passed through LSTM layers, while the correlation matrixes are processed by GraphConvolution layers. They are implemented in Spektral, a cool library for graph deep learning build on Tensorflow. It has various kinds of graph layers available. We use the most basic one, the GraphConvolution. It operates a series of convolution operations between learnable weights, external node features (provided together with the adjacent matrix), and our correlation matrixes. Unlikely, at the moment Spektral doesn’t support Window so I have to extract manually the class of my interest and create my Python executable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    \n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    \n",
    "    set_seed(33)\n",
    "\n",
    "    opt = Adam(lr=0.001)\n",
    "\n",
    "    inp_seq = Input((sequence_length, 10))\n",
    "    inp_lap = Input((10, 10))\n",
    "    inp_feat = Input((10, X_train_feat.shape[-1]))\n",
    "\n",
    "    x = GraphConv(32, activation='relu')([inp_feat, inp_lap])\n",
    "    x = GraphConv(16, activation='relu')([x, inp_lap])\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    xx = LSTM(128, activation='relu', return_sequences=True)(inp_seq)\n",
    "    xx = LSTM(32, activation='relu')(xx)\n",
    "\n",
    "    x = Concatenate()([x,xx])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    out = Dense(1)(x)\n",
    "\n",
    "    model = Model([inp_seq, inp_lap, inp_feat], out)\n",
    "    model.compile(optimizer=opt, loss='mse', \n",
    "                  metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TRAIN A MODEL FOR EACH STORES USING ALL THE DATA AVAILALBE FROM OTHER STORES ###\n",
    "\n",
    "pred_valid_all = np.zeros(y_valid.shape)\n",
    "pred_test_all = np.zeros(y_test.shape)\n",
    "\n",
    "for store in range(10):\n",
    "\n",
    "    print('-------', 'store', store, '-------')\n",
    "    \n",
    "    es = EarlyStopping(patience=5, verbose=1, min_delta=0.001, monitor='val_loss', mode='auto', restore_best_weights=True)\n",
    "\n",
    "    model = get_model()\n",
    "    model.fit([X_train_seq, X_train_lap, X_train_feat], y_train[:,store], epochs=100, batch_size=256, \n",
    "              validation_data=([X_valid_seq, X_valid_lap, X_valid_feat], y_test[:,store]), callbacks=[es], verbose=2)\n",
    "\n",
    "    pred_valid_all[:,store] = model.predict([X_valid_seq, X_valid_lap, X_valid_feat]).ravel()\n",
    "    pred_test_all[:,store] = model.predict([X_test_seq, X_test_lap, X_test_feat]).ravel()\n",
    "\n",
    "\n",
    "pred_valid_all = scaler_seq.inverse_transform(pred_valid_all)\n",
    "reverse_valid = scaler_seq.inverse_transform(y_valid)\n",
    "pred_test_all = scaler_seq.inverse_transform(pred_test_all)\n",
    "reverse_test = scaler_seq.inverse_transform(y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train is computed with the first two years of data while the remaining two are respectively used for validation and testing. I trained a model for each store so we ended with a total of 10 different neural networks.\n",
    "\n",
    "The predictions of stores are retrieved at the end of the training procedure by the relative models. The errors are calculated as RMSE on test data and reported below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RMSE ON TEST DATA ###\n",
    "\n",
    "error = {}\n",
    "\n",
    "for store in range(10):\n",
    "    \n",
    "    error[store] = np.sqrt(mean_squared_error(reverse_test[:,store], pred_test_all[:,store]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PLOT RMSE ###\n",
    "\n",
    "plt.figure(figsize=(14,5))\n",
    "plt.bar(range(10), error.values())\n",
    "plt.xticks(range(10), ['store_'+str(s) for s in range(10)])\n",
    "plt.ylabel('error')\n",
    "np.set_printoptions(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### UTILITY FUNCTION TO PLOT PREDICTION ###\n",
    "\n",
    "def plot_predictions(y_true, y_pred, store, item):\n",
    "    \n",
    "    y_true = y_true.reshape(50,-1,10)\n",
    "    y_pred = y_pred.reshape(50,-1,10)\n",
    "    \n",
    "    plt.plot(y_true[item,:,store], label='true')\n",
    "    plt.plot(y_pred[item,:,store], label='prediction')\n",
    "    plt.title(f\"store: {store} item: {item}\"); plt.legend()\n",
    "    plt.ylabel('sales'); plt.xlabel('date')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same way, it’s easy to extract the predictions for items in desired stores directing manipulating our nested data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11,5))\n",
    "plot_predictions(reverse_test, pred_test_all, 7,0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly Detection in Multivariate Time Series in Networks Graphs\n",
    "\n",
    "When working on an anomaly detection task, we are used to discovering and pointing out situations where the data register unseen dynamics. The ability to study the past and extrapolate a “normal” behavior is crucial for the success of most anomaly detection applications. In this situation, an adequate learning strategy must take into consideration the temporal dependency. What in the past may be considered “anomalous”, now may be marked as “normal” since the underlying data dynamics are prone to change.\n",
    "\n",
    "A good multivariate anomaly detection system should study the historical relationship between data and alert possible future divergencies. Given these premises, the PCA (Principal Component Analysis) algorithm reveals to be a very good candidate for the task. Together with the old but gold PCA, we can leverage the connected nature of the signals at our disposal to develop a network structure and detect anomalies with a graph-based approach.\n",
    "\n",
    "In this post, we carry out a multivariate anomaly detection task adopting an unsupervised approach based on network clustering. We build a network graph from a bunch of time series looking at their correlation. Then we apply, on top of the correlation matrix, the DBSCAN algorithm to identify potential anomalous patterns."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data\n",
    "\n",
    "We imagine working in a multivariate system. All the series show a positive correlation degree between each other while maintaining stationery for the whole period of reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy.cluster.hierarchy as sch\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We manually insert in our data an anomalous period. We change a sequential bunch of observations, in a predefined feature, with some gaussian noise. We do this “swapping” operation taking care to maintain the same mean and standard deviation of the original data. In this way, the anomalous period is not visible simply by looking at the feature distributions. In other words, we alter the correlation present in the data, in a given time range, by inserting some gaussian noise that is not related to anything in the entire system.\n",
    "\n",
    "Our scope is to develop an anomaly detection solution that can correctly point out the changes in feature relationships. Let's start adopting an approach based on PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SYNTHETIC CORRELATED DATA GENERATION ###\n",
    "\n",
    "np.random.seed(33)\n",
    "\n",
    "n_features = 50\n",
    "n_samples = 4_000\n",
    "\n",
    "r = np.random.randint(-2,10, (n_features,n_features))\n",
    "r = np.dot(r, r.T)\n",
    "\n",
    "x = np.random.normal(0,1, (n_features,n_samples))\n",
    "c = np.linalg.cholesky(r)\n",
    "\n",
    "X = pd.DataFrame(\n",
    "    np.dot(c, x).T, columns=[f\"feat_{c+1}\" for c in range(n_features)]\n",
    ")\n",
    "\n",
    "corr_mat = pairwise_distances(X.T, metric='correlation')\n",
    "\n",
    "X.shape, corr_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PLOT SERIES DISTRIBUTIONS ###\n",
    "\n",
    "plt.figure(figsize=(18,6))\n",
    "plt.subplot(1,2,1)\n",
    "X.plot(legend=False, xlabel='timesteps', ax=plt.gca())\n",
    "plt.subplot(1,2,2)\n",
    "X.plot(kind='density', legend=False, ax=plt.gca())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PLOT SERIES CORRELATIONS ###\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(X.corr(), annot=False, cmap='bwr')\n",
    "plt.title('correlation matrix')\n",
    "plt.ylabel('series'); plt.xlabel('series')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MANUALLY INSERT ANOMALOUS PERIOD ###\n",
    "\n",
    "n_samples_change = 400\n",
    "_ = np.random.randint(0,n_samples-n_samples_change)\n",
    "\n",
    "X.loc[np.arange(_, _+n_samples_change), 'feat_2'] = \\\n",
    "    np.random.normal(X['feat_2'].mean(), X['feat_2'].std(), (n_samples_change,))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Anomaly Detection\n",
    "\n",
    "PCA fits particularly well in our scenario. This is not surprising since it’s widely adopted in a lot of industrial applications for anomaly detection. Its adaptability and flexibility make it the standard solution when needing to detect anomalies in a multivariate system.\n",
    "\n",
    "Detecting anomalies with PCA is straightforward. We leverage the compression ability of PCA to reduce the original feature dimensionality of the data. In this way, we preserve only meaningful interactions while reducing the number of features and discharging the noise. The dimensionality reduction operation is fully reversible. We can come back to the original data shape effortlessly using only the learned relationships in the original set of data.\n",
    "\n",
    "We can calculate the reconstruction error to see and summarize the goodness of the compression operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PCA ANOMALY DETECTION ###\n",
    "\n",
    "rec_errors_samples = {}\n",
    "rec_errors_features = {}\n",
    "\n",
    "for i, (past_id,future_id) in enumerate(\n",
    "    TimeSeriesSplit(10, test_size=300).split(X)\n",
    "):\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    pca = PCA(0.7, random_state=33)\n",
    "    pca.fit(scaler.fit_transform(X.iloc[past_id]))\n",
    "    \n",
    "    Xt = pca.inverse_transform(\n",
    "        pca.transform(\n",
    "            scaler.transform(X.iloc[future_id])\n",
    "        )\n",
    "    )\n",
    "    rec_errors_samples[past_id[-1]] = \\\n",
    "        np.linalg.norm(scaler.transform(X.iloc[future_id]) - Xt, axis=1)\n",
    "    rec_errors_features[past_id[-1]] = \\\n",
    "        np.linalg.norm(scaler.transform(X.iloc[future_id]) - Xt, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PLOT PCA RECONSTRUCTION ERRORS ###\n",
    "\n",
    "plt.figure(figsize=(18,6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(list(rec_errors_samples.keys()), \n",
    "         [np.mean(r) for r in rec_errors_samples.values()])\n",
    "plt.ylabel('recontruction error'); plt.xlabel('timesteps')\n",
    "plt.title('sample-wise recontruction')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "for i in range(n_features):\n",
    "    rec = []\n",
    "    for r in rec_errors_features.values():\n",
    "        rec.append(r[i])\n",
    "    plt.plot(list(rec_errors_features.keys()), rec)\n",
    "plt.ylabel('recontruction error'); plt.xlabel('timesteps')\n",
    "plt.title('feature-wise recontruction')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the reconstruction error as an anomaly score. A high magnitude of the reconstruction error means that a change in the data relationships has happened. We can compute it sample-wise to attribute an anomaly score to each sample. We can also compute it for each feature over time and observe which feature is affected by an anomaly behavior. In both cases, we follow a temporal validation strategy to simulate the real data flow in a time-dependent system.\n",
    "\n",
    "In our simulation study, we see how the reconstruction error increase in correspondence with the artificial anomalous period (both samples and feature-wise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### COMPUTE HIERARCHICAL CLUSTERING ON CORRELATION MATRIX ###\n",
    "\n",
    "d = sch.distance.pdist(corr_mat)\n",
    "L = sch.linkage(d, method='ward')\n",
    "ind = sch.fcluster(L, d.max(), 'distance')\n",
    "dendrogram = sch.dendrogram(L, no_plot=True)\n",
    "\n",
    "labels = dendrogram['leaves']\n",
    "corr_mat_cluster = pairwise_distances(\n",
    "    pd.concat([X.iloc[:,[i]] for i in labels], axis=1).T,\n",
    "    metric='correlation'\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(18,8))\n",
    "plt.subplot(1,2,1)\n",
    "dendrogram = sch.dendrogram(L, no_plot=False)\n",
    "plt.title('dendrogram')\n",
    "plt.ylabel('distance'); plt.xlabel('series')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(corr_mat_cluster, cmap='bwr')\n",
    "plt.title('correlation matrix')\n",
    "plt.ylabel('series'); plt.xlabel('series')\n",
    "plt.xticks(range(n_features), labels, rotation=90)\n",
    "plt.yticks(range(n_features), labels)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BDSCAN Anomaly Detection\n",
    "\n",
    "To develop our graph-based methodology, as a first step, we simply have to divide the data into temporal windows of the same size.\n",
    "\n",
    "Then we should compute the correlation of the series in each time window. The correlation matrix is used as an approximation of the internal system dynamics and the relationships between series. Other similarity measures can work well like euclidean distance or dynamic time warping.\n",
    "\n",
    "As a final step, we fit a DBSCAN algorithm on each similarity matrix. In a normal situation, we should expect all the series to belong to the same cluster. In case of anomalies, we expect to have multiple clusters of series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DBSCAN ANOMALY DETECTION ###\n",
    "\n",
    "network_ano = {}\n",
    "dbscan = DBSCAN(eps=0.6, min_samples=1, metric=\"precomputed\")\n",
    "\n",
    "for i, (past_id,_) in enumerate(\n",
    "    TimeSeriesSplit(10, test_size=300, max_train_size=300).split(X)\n",
    "):\n",
    "    \n",
    "    preds = dbscan.fit_predict(\n",
    "        pairwise_distances(X.iloc[past_id].T, metric='correlation')\n",
    "    )\n",
    "    if (preds > 0).any():\n",
    "        ano_features = list(X.columns[np.where(preds > 0)[0]])\n",
    "        network_ano[past_id[-1]] = ano_features\n",
    "    else:\n",
    "        network_ano[past_id[-1]] = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlated series are supposed to move together. If one of them changes its direction, concerning the others, we can mark it as anomalous. To get the best from this methodology, it’s important to work with series that show the same correlation degrees. That it’s not always possible due to the complex nature of some systems. In these cases, a preliminary clustering should be made. A simple hierarchical clustering may be good to group the series according to their nature and enable DBSCAN to achieve the best performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PLOT DBSCAN DETECTED ANOMALIES ###\n",
    "\n",
    "roll_corr = X.rolling(300).corr()\n",
    "\n",
    "for ano_loc,ano in network_ano.items():\n",
    "    if ano is not None:\n",
    "        for ano_feat in network_ano[ano_loc]:\n",
    "            roll_corr[ano_feat].unstack().plot(\n",
    "                legend=False, figsize=(11,6),\n",
    "                title=f\"{ano_feat} rolling correlation\",\n",
    "                ylabel='correlation', xlabel='timesteps'\n",
    "            )\n",
    "            plt.axvline(ano_loc, linestyle='--', c='black')\n",
    "            plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, the entire procedure detects the change in the internal system dynamic in correspondence with the anomalous period."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
